[nltk_data] Downloading package averaged_perceptron_tagger to
[nltk_data]     /home/pratyush/nltk_data...
[nltk_data]   Package averaged_perceptron_tagger is already up-to-
[nltk_data]       date!
---------- Loading the data ---------- 
Data filename :  ../Data/data_window_ngram-5.pkl

Training data size :  229163 , Test data size :  138880
---------- Pre-processing ----------

---------- Pre-processing ----------

---------- Random Forest ----------
---------- Training Phase ----------
---------- Generating features ---------- 
Total number of features :  151 

Class Distribution of training data :  (array([0, 1]), array([29729,  2163]))
---------- Building model ----------
---------- Evaluation performance on training data ----------
accuracy :  0.9940110372507212
Precision :  0.9722222222222222
Recall :  0.9385113268608414
f1-score :  0.9550693954363679

---------- Testing Phase ----------
---------- Generating features ---------- 
Total number of features :  151 

---------- Evaluation performance on test data ----------
Class Distribution of test data :  (array([0, 1]), array([19295,  1275])) 

accuracy :  0.9670393777345649
Precision :  0.7946692991115498
Recall :  0.6313725490196078
f1-score :  0.7036713286713286

---------- Decision Tree Classifier ----------
---------- Training Phase ----------
---------- Generating features ---------- 
Total number of features :  151 

Class Distribution of training data :  (array([0, 1]), array([29729,  2163]))
---------- Building model ----------
---------- Evaluation performance on training data ----------
accuracy :  0.9940110372507212
Precision :  0.9819159335288368
Recall :  0.9288025889967637
f1-score :  0.9546210501306723

---------- Testing Phase ----------
---------- Generating features ---------- 
Total number of features :  151 

---------- Evaluation performance on test data ----------
Class Distribution of test data :  (array([0, 1]), array([19295,  1275])) 

accuracy :  0.9578998541565387
Precision :  0.661150512214342
Recall :  0.6580392156862745
f1-score :  0.6595911949685536

